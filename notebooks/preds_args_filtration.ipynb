{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lucene\n",
    "import shelve\n",
    "import random\n",
    "import itertools\n",
    "import jsonlines\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from java.nio.file import Paths\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.index import Term, DirectoryReader, IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher, Query, BoostQuery, BooleanQuery, BooleanClause, TermQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(field: str, tokens: List[str]) -> Query:\n",
    "    query_builder = BooleanQuery.Builder()\n",
    "    for token in tokens:\n",
    "        query_builder.add(TermQuery(Term(field, token)), BooleanClause.Occur.SHOULD)\n",
    "    return query_builder.build()\n",
    "\n",
    "\n",
    "def get_combined_query(fields: List[str], tokens_list: List[List[str]],\n",
    "                       weights: List[float], is_mandatory: List[bool]) -> Query:\n",
    "    assert len(fields) == len(tokens_list) == len(weights) == len(is_mandatory)\n",
    "    query_builder = BooleanQuery.Builder()\n",
    "    for field, tokens, weight, flag in zip(fields, tokens_list, weights, is_mandatory):\n",
    "        occur = BooleanClause.Occur.MUST if flag else BooleanClause.Occur.SHOULD\n",
    "        query_builder.add(BoostQuery(get_query(field, tokens), weight), occur)\n",
    "    return query_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index CGW texts (for debugging only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1454306it [02:32, 9540.44it/s] \n",
      "127505it [00:13, 9251.69it/s]\n",
      "163083it [00:17, 9526.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('../data/cgw/shelve/cgw.shelve') as db:\n",
    "    for split_name in ['train', 'test', 'dev']:\n",
    "        with jsonlines.open(f'../data/cgw/jsonl/{split_name}.jsonl') as reader:\n",
    "            for doc in tqdm(reader):\n",
    "                db[doc['id']] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command-line args\n",
    "\n",
    "index_dir = '../data/cgw/preds_args/lucene_index'\n",
    "schema_path = '../data/schemas/descrs/election.txt'\n",
    "p_w = 1.0\n",
    "a_w = 1.0\n",
    "pa_w = 5.0\n",
    "p_must = True\n",
    "a_must = True\n",
    "pa_must = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lucene and the JVM\n",
    "lucene.initVM()\n",
    "directory = SimpleFSDirectory.open(Paths.get(index_dir))\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index contains 1744894 docs\n"
     ]
    }
   ],
   "source": [
    "print(f'Index contains {searcher.getIndexReader().numDocs()} docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema \"ce_040_arrest\" has 4979 relevant docs and 2489 related docs\n",
      "Schema \"ce_001_protest\" has 1876 relevant docs and 1500 related docs\n",
      "Schema \"ce_005_disease_outbreak\" has 3965 relevant docs and 1982 related docs\n",
      "Schema \"plane_crash\" has 1974 relevant docs and 1579 related docs\n",
      "Schema \"election\" has 25521 relevant docs and 12760 related docs\n"
     ]
    }
   ],
   "source": [
    "# Retrieve schema-related docs\n",
    "\n",
    "all_relevant_docs_ids = set()\n",
    "schemas_relevant_docs_ids = {}\n",
    "schemas_related_docs_ids = {}\n",
    "\n",
    "for schema_name in ['ce_040_arrest', 'ce_001_protest', 'ce_005_disease_outbreak',\n",
    "                    'plane_crash', 'election']:\n",
    "    schema_path = f'../data/schemas/descrs/{schema_name}.txt'\n",
    "    # Read the schema\n",
    "    combined_tokens = [set(), # preds\n",
    "                       set(), # args\n",
    "                       set(), # preds_args\n",
    "                      ]\n",
    "    with open(schema_path) as fin:\n",
    "        for line in fin:\n",
    "            arg0, arg1, pred_idx = line.strip().split()\n",
    "            pred_idx = int(pred_idx)\n",
    "            assert pred_idx in range(2)\n",
    "            # Wikidata refs are ignored for now\n",
    "            arg0 = arg0.split(':')[0]\n",
    "            arg1 = arg1.split(':')[0]\n",
    "            pred = [arg0, arg1][pred_idx]\n",
    "            arg = [arg0, arg1][1 - pred_idx]\n",
    "            combined_tokens[0].add(pred)\n",
    "            combined_tokens[1].add(arg)\n",
    "            combined_tokens[2].add(f'{arg0}_{arg1}_{pred_idx}')\n",
    "            \n",
    "    # Query the docs\n",
    "    query = get_combined_query(['preds', 'args', 'preds_args'], combined_tokens,\n",
    "                               [p_w, a_w, pa_w], [p_must, a_must, pa_must])\n",
    "    num_relevant_docs = searcher.count(query)\n",
    "    num_related_docs = int(0.80 * num_relevant_docs if num_relevant_docs < 3000 else 0.5 * num_relevant_docs)\n",
    "    print(f'Schema \"{schema_name}\" has {num_relevant_docs} relevant docs and {num_related_docs} related docs')\n",
    "    relevant_docs_ids = [searcher.doc(d.doc).get('filename') for d in searcher.search(query, num_relevant_docs).scoreDocs]\n",
    "    assert all(x.endswith('.comm') for x in relevant_docs_ids)\n",
    "    relevant_docs_ids = [x[:-5] for x in relevant_docs_ids]\n",
    "    related_docs_ids = relevant_docs_ids[:num_related_docs]\n",
    "    schemas_relevant_docs_ids[schema_name] = relevant_docs_ids\n",
    "    schemas_related_docs_ids[schema_name] = related_docs_ids\n",
    "    all_relevant_docs_ids.update(relevant_docs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 1.24 s, total: 23.7 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Sample schema-unrelated docs ids\n",
    "# Retrieve schema-related and unrelated docs\n",
    "\n",
    "schemas_related_docs = {}\n",
    "schemas_unrelated_docs = {}\n",
    "\n",
    "rnd = random.Random(0)\n",
    "\n",
    "with shelve.open('../data/cgw/shelve/cgw.shelve') as db:\n",
    "    all_docs_ids = list(db.keys())\n",
    "\n",
    "    for schema_name, related_docs_ids in schemas_related_docs_ids.items():\n",
    "        # Sample schema-unrelated docs ids\n",
    "        unrelated_docs_ids = set()\n",
    "        while len(unrelated_docs_ids) != len(related_docs_ids):\n",
    "            sample_doc_id = rnd.choice(all_docs_ids)\n",
    "            if sample_doc_id not in all_relevant_docs_ids:\n",
    "                unrelated_docs_ids.add(sample_doc_id)\n",
    "\n",
    "        # Retrieve schema-related and unrelated docs\n",
    "        schemas_related_docs[schema_name] = [db[doc_id] for doc_id in related_docs_ids]\n",
    "        schemas_unrelated_docs[schema_name] = [db[doc_id] for doc_id in unrelated_docs_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write schema-related docs\n",
    "for schema_name, docs in schemas_related_docs.items():\n",
    "    with jsonlines.open(f'../data/cgw/schema_related/pos/{schema_name}.jsonl', 'w') as writer:\n",
    "        writer.write_all(docs)\n",
    "\n",
    "# Write schema-unrelated docs\n",
    "for schema_name, docs in schemas_unrelated_docs.items():\n",
    "    with jsonlines.open(f'../data/cgw/schema_related/neg/{schema_name}.jsonl', 'w') as writer:\n",
    "        writer.write_all(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write schema-related docs ids\n",
    "\n",
    "all_schema_related_docs_ids = list(itertools.chain.from_iterable((doc['id'] for doc in docs) for docs in schemas_related_docs.values()))\n",
    "with open(f'../data/cgw/schema_related/pos/docs_ids.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(all_schema_related_docs_ids))\n",
    "\n",
    "# Write schema-unrelated docs ids\n",
    "all_schema_unrelated_docs_ids = list(itertools.chain.from_iterable((doc['id'] for doc in docs) for docs in schemas_unrelated_docs.values()))\n",
    "with open(f'../data/cgw/schema_related/neg/docs_ids.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(all_schema_unrelated_docs_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-project",
   "language": "python",
   "name": "causal-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
